{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03abff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/zhewen/.conda/envs/gsn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import urllib.request\n",
    "import imageio as imio\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, HTML, display\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.model_utils import TrajectorySampler\n",
    "from builders.builders import build_dataloader\n",
    "from utils.utils import instantiate_from_config\n",
    "from notebooks.walkthrough_utils import get_smooth_trajectory\n",
    "from utils.camera_trajectory import rotate_n, go_forward, go_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde26aaf",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabb8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('replica_128x128.ckpt', <http.client.HTTPMessage at 0x2ba830f41970>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download pretrained model (takes anywhere from a few seconds to a few minutes depending on internet speed)\n",
    "model_url = 'https://docs-assets.developer.apple.com/ml-research/models/gsn/replica_128x128.ckpt'\n",
    "checkpoint_filename = 'replica_128x128.ckpt'\n",
    "urllib.request.urlretrieve(model_url, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: adjust the data path here if data is in a different location than default\n",
    "data_path = '../data/replica_all'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# get rid of all the inception params which are leftover from FID metric\n",
    "keys_for_deletion = []\n",
    "for key in state_dict.keys():\n",
    "    if 'fid' in key:\n",
    "        keys_for_deletion.append(key)\n",
    "for key in keys_for_deletion:\n",
    "    del state_dict[key]\n",
    "\n",
    "opt = checkpoint['opt']\n",
    "opt.data_config.data_dir = data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57db11fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/replica_all/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create dataloader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_module \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# collect a set of real trajectories from the dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m real_Rts \u001b[38;5;241m=\u001b[39m data_module\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mget_trajectory_Rt()\n",
      "File \u001b[0;32m/scratch/user/zhewen/ml-gsn/notebooks/../builders/builders.py:32\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[0;34m(data_config, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m train_set_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_loader_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_config\u001b[38;5;241m.\u001b[39mtrain_set_config}\n\u001b[1;32m     29\u001b[0m train_set_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     k: train_set_config[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m train_set_config \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_set_config\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_set_config\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 32\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_set_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# get only the args for the DataLoader class, since it can't deal with extra args\u001b[39;00m\n\u001b[1;32m     34\u001b[0m train_loader_config \u001b[38;5;241m=\u001b[39m {k: train_set_config[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m data_loader_args}\n",
      "File \u001b[0;32m/scratch/user/zhewen/ml-gsn/notebooks/../datasets/replica.py:41\u001b[0m, in \u001b[0;36mReplicaDataset.__init__\u001b[0;34m(self, data_dir, split, seq_len, step, img_res, depth, center, normalize_rotation, rot_aug, single_sample_per_trajectory, samples_per_epoch, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter \u001b[38;5;241m=\u001b[39m center\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_rotation \u001b[38;5;241m=\u001b[39m normalize_rotation\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mlistdir_nohidden\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatapath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrot_aug \u001b[38;5;241m=\u001b[39m rot_aug\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_sample_per_trajectory \u001b[38;5;241m=\u001b[39m single_sample_per_trajectory\n",
      "File \u001b[0;32m/scratch/user/zhewen/ml-gsn/notebooks/../datasets/dataset_utils.py:7\u001b[0m, in \u001b[0;36mlistdir_nohidden\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlistdir_nohidden\u001b[39m(path):\n\u001b[0;32m----> 7\u001b[0m     mylist \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mylist\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/replica_all/train'"
     ]
    }
   ],
   "source": [
    "# create dataloader\n",
    "data_module = build_dataloader(opt.data_config, verbose=False)\n",
    "\n",
    "# collect a set of real trajectories from the dataset\n",
    "real_Rts = data_module.train_loader.dataset.get_trajectory_Rt()\n",
    "trajectory_sampler = TrajectorySampler(real_Rts=real_Rts, mode=opt.model_config.params.trajectory_mode)\n",
    "\n",
    "# initialize model and load state\n",
    "gsn = instantiate_from_config(opt.model_config).cuda().eval()\n",
    "gsn.set_trajectory_sampler(trajectory_sampler=trajectory_sampler)\n",
    "gsn.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# increase nerf_out_res after loading model to generate higher res samples (i.e., implicit upsampling)\n",
    "gsn.generator_config.params.nerf_out_res *= 2\n",
    "# trade render speed for memory by rendering smaller patches at a time\n",
    "gsn.patch_size = 32\n",
    "\n",
    "# load a batch of data so that we can use the camera parameters\n",
    "real_data = next(iter(data_module.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fd1fb",
   "metadata": {},
   "source": [
    "## Interactive Walkthrough\n",
    "Use wasd to explore the scene. Enter 'q' to stop exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f730fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "z_dim = opt.model_config.params.decoder_config.params.z_dim\n",
    "z = torch.randn(1, z_dim, device=device)\n",
    "\n",
    "# intrinsic camera matrix from real data\n",
    "K_current = real_data['K'].to(device)\n",
    "# initialize extrinsic camera matrix at the center of the scene\n",
    "Rt_current = torch.eye(4, device=device).view(1, 1, 4, 4)\n",
    "\n",
    "trajectory = {'rgb': [], 'depth': [], 'Rt': [], 'K': []}\n",
    "\n",
    "while True:\n",
    "    camera_params = {'K': K_current, 'Rt': Rt_current}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_rgb, fake_depth, Rt, K = gsn(z, camera_params=camera_params)\n",
    "    \n",
    "    trajectory['rgb'].append(fake_rgb)\n",
    "    trajectory['depth'].append(fake_depth)\n",
    "    trajectory['Rt'].append(Rt)\n",
    "    trajectory['K'].append(K)\n",
    "    \n",
    "    rgb_current = rearrange(fake_rgb, 'b t c h w -> (b t h) w c').cpu()\n",
    "    \n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize = (8, 8)) \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(rgb_current, interpolation='bilinear')\n",
    "    ax.set_title('Current observation');\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    action = input(\"a/w/d/s\")\n",
    "    if action == 'a':\n",
    "        # Turn left\n",
    "        Rt = rotate_n(n=-30.0).to(device)\n",
    "        Rt_current = torch.bmm(Rt.unsqueeze(0), Rt_current[0]).unsqueeze(0)\n",
    "    if action == 'd':\n",
    "        # Turn right\n",
    "        Rt = rotate_n(n=30.0).to(device)\n",
    "        Rt_current = torch.bmm(Rt.unsqueeze(0), Rt_current[0]).unsqueeze(0)\n",
    "    if action == 'w':\n",
    "        # Go forward\n",
    "        Rt_current = go_forward(Rt_current, step=opt.model_config.params.voxel_size / 0.6)\n",
    "    if action == 's':\n",
    "        # Go backward\n",
    "        Rt_current = go_backward(Rt_current, step=opt.model_config.params.voxel_size / 0.6)\n",
    "    if action == 'q':\n",
    "        break\n",
    "        \n",
    "for key in trajectory.keys():\n",
    "    trajectory[key] = torch.cat(trajectory[key], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3603cd5",
   "metadata": {},
   "source": [
    "## Generate Animation\n",
    "Fit a spline to the camera trajectory and uniformly render frames along it to produce a smooth animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jitter camera pose a tiny amount to make sure each pose is unique\n",
    "# (to avoid problems with trajectory smoothing)\n",
    "trajectory['Rt'] = trajectory['Rt'] + torch.rand_like(trajectory['Rt']) * 1e-5\n",
    "\n",
    "# fit a smooth spline to the trajectory keypoints\n",
    "n_keypoints = len(trajectory['Rt'][0])\n",
    "new_Rts = get_smooth_trajectory(Rt=trajectory['Rt'][0], n_frames=5 * n_keypoints, subsample=3)\n",
    "n_steps = len(new_Rts)\n",
    "\n",
    "# render frames along new smooth trajectory\n",
    "resampled_trajectory = {'rgb': [], 'depth': [], 'Rt': [], 'K': []}\n",
    "for i in range(n_steps):\n",
    "    if i % 10 == 0:\n",
    "        print('Rendering frame {}/{}'.format(i, n_steps))\n",
    "    \n",
    "    camera_params = {'K': K_current[:1, :1], 'Rt': new_Rts[i:i+1].to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_rgb, fake_depth, Rt, K = gsn(z, camera_params=camera_params)\n",
    "\n",
    "    resampled_trajectory['rgb'].append(fake_rgb)\n",
    "    resampled_trajectory['depth'].append(fake_depth)\n",
    "    resampled_trajectory['Rt'].append(Rt)\n",
    "    resampled_trajectory['K'].append(K)\n",
    "\n",
    "for key in resampled_trajectory.keys():\n",
    "    resampled_trajectory[key] = torch.cat(resampled_trajectory[key], dim=1)\n",
    "\n",
    "imgs = []\n",
    "for i in range(n_steps):\n",
    "    im = resampled_trajectory['rgb'][0, i].permute(1, 2, 0).detach().cpu()\n",
    "    imgs.append((im * 255).byte())\n",
    "\n",
    "# save gif with random unique name\n",
    "n = ''.join(random.choice('0123456789ABCDEF') for i in range(5))\n",
    "animation_filename = 'camera_trajectory_{}.gif'.format(n)\n",
    "print('Saving animation to {}\\n'.format(animation_filename))\n",
    "imio.mimsave(animation_filename, imgs, fps=30)\n",
    "\n",
    "display(HTML('<img src={}>'.format(animation_filename)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
